
//  THREAD PARAMETERS

int bx = item.get_group(0);    // get current horizontal block index (0-n)
int tx = item.get_local_id(0); // get current horizontal thread index (0-n)
int wtx = tx;

//  DO FOR THE NUMBER OF BOXES

if(bx<dim_cpu_number_boxes) {

  //  Extract input parameters

  // parameters
  fp a2 = 2*par_cpu.alpha*par_cpu.alpha;

  // home box
  int first_i;
  // (enable the line below only if wanting to use shared memory)

  // nei box
  int pointer;
  int k = 0;
  int first_j;
  int j = 0;
  // (enable the two lines below only if wanting to use shared memory)

  // common
  fp r2;
  fp u2;
  fp vij;
  fp fs;
  fp fxij;
  fp fyij;
  fp fzij;
  THREE_VECTOR d;

  //  Home box

  //  Setup parameters

  // home box - box parameters
  first_i = d_box_gpu_acc[bx].offset;

  //  Copy to shared memory

  // (enable the section below only if wanting to use shared memory)
  // home box - shared memory
  while(wtx<NUMBER_PAR_PER_BOX){
    rA_shared[wtx] = d_rv_gpu_acc[first_i+wtx];
    wtx = wtx + NUMBER_THREADS;
  }
  wtx = tx;

  // (enable the section below only if wanting to use shared memory)
  // synchronize threads  - not needed, but just to be safe for now
  item.barrier(access::fence_space::local_space);

  //  nei box loop

  // loop over nei boxes of home box
  for (k=0; k<(1+d_box_gpu_acc[bx].nn); k++){

    //  nei box - get pointer to the right box

    if(k==0){
      pointer = bx;                          // set first box to be processed to home box
    }
    else{
      pointer = d_box_gpu_acc[bx].nei[k-1].number;              // remaining boxes are nei boxes
    }

    //  Setup parameters

    // nei box - box parameters
    first_j = d_box_gpu_acc[pointer].offset;

    // (enable the section below only if wanting to use shared memory)
    // nei box - shared memory
    while(wtx<NUMBER_PAR_PER_BOX){
      rB_shared[wtx] = d_rv_gpu_acc[first_j+wtx];
      qB_shared[wtx] = d_qv_gpu_acc[first_j+wtx];
      wtx = wtx + NUMBER_THREADS;
    }
    wtx = tx;

    // (enable the section below only if wanting to use shared memory)
    // synchronize threads because in next section each thread accesses data brought in by different threads here
    item.barrier(access::fence_space::local_space);

    //  Calculation

    // loop for the number of particles in the home box
    while(wtx<NUMBER_PAR_PER_BOX){

      // loop for the number of particles in the current nei box
      for (j=0; j<NUMBER_PAR_PER_BOX; j++){

        r2 = rA_shared[wtx].v + rB_shared[j].v - DOT(rA_shared[wtx],rB_shared[j]); 
        u2 = a2*r2;
        vij= cl::sycl::exp(-u2);
        fs = 2*vij;
        d.x = rA_shared[wtx].x  - rB_shared[j].x;
        fxij=fs*d.x;
        d.y = rA_shared[wtx].y  - rB_shared[j].y;
        fyij=fs*d.y;
        d.z = rA_shared[wtx].z  - rB_shared[j].z;
        fzij=fs*d.z;
        d_fv_gpu_acc[first_i+wtx].v +=  qB_shared[j]*vij;
        d_fv_gpu_acc[first_i+wtx].x +=  qB_shared[j]*fxij;
        d_fv_gpu_acc[first_i+wtx].y +=  qB_shared[j]*fyij;
        d_fv_gpu_acc[first_i+wtx].z +=  qB_shared[j]*fzij;
      }

      // increment work thread index
      wtx = wtx + NUMBER_THREADS;
    }

    // reset work index
    wtx = tx;

    // synchronize after finishing force contributions from current nei box not to cause conflicts when starting next box
    item.barrier(access::fence_space::local_space);

    //  Calculation END
  }
  //  nei box loop END
}
